{"text": "# SLM-Boot: Offline AI Assistant System Overview\n\nSLM-Boot is a NixOS-based bootable system designed for running local AI assistants completely offline. It uses Nix flakes for reproducible builds and supports multiple deployment formats.\n\n## Key Features\n- **Multiple deployment formats**: Graphical live USB ISO, headless kexec bundles, VM images (qcow2), and raw disk images\n- **CachyOS BORE kernel**: Enhanced for better interactivity and responsiveness with AI workloads\n- **SLM-Assist**: Local DSPy RAG system with Ollama backend and Gradio web interface\n- **Delayed startup**: Configurable delays ensure services initialize properly before the UI launches\n- **Auto-browser launch**: Automatic Floorp browser opening on graphical profiles\n- **Corpus integration**: Pre-baked knowledge base for RAG queries\n\n## Architecture\nThe system consists of:\n1. Base NixOS configuration modules (graphical/headless)\n2. Container runtime (Podman) for isolated services\n3. Ollama LLM server with pre-pulled models\n4. SLM-Assist RAG application with DSPy framework\n5. Optional OpenWebUI for model interaction\n6. Production hardening and optimization modules", "source": "system_overview", "chunk_index": 0, "total_chunks": 1}
{"text": "# SLM-Boot Flake Structure\n\nThe flake.nix file defines the entire system configuration using Nix flakes.\n\n## Inputs\n```nix\ninputs = {\n  nixpkgs.url = \"github:NixOS/nixpkgs/nixos-unstable\";\n  nixos-generators = {\n    url = \"github:nix-community/nixos-generators\";\n    inputs.nixpkgs.follows = \"nixpkgs\";\n  };\n};\n```\n\n- **nixpkgs**: Uses the unstable channel for latest packages\n- **nixos-generators**: Provides utilities for generating various output formats (ISO, kexec, qcow2, raw)\n\n## Module Organization\nModules are organized in the `modules/` directory:\n- `graphical-minimal.nix`: Minimal desktop environment (DWM)\n- `headless-minimal.nix`: Server configuration without GUI\n- `slm-assist/default.nix`: Main SLM-Assist RAG system\n- `ollama-service.nix`: Ollama LLM server (legacy, now integrated in slm-assist)\n- `open-webui-service.nix`: Web UI for model interaction\n- `kernel-cachyos-bore.nix`: BORE kernel configuration\n- `hardening.nix`: Security hardening\n- `production-extras.nix`: Performance optimizations", "source": "flake_structure", "chunk_index": 0, "total_chunks": 1}
{"text": "# SLM-Boot Build Targets\n\nThe flake provides multiple build targets for different use cases.\n\n## Graphical ISO (Live USB)\n```bash\nnix build .#packages.x86_64-linux.graphical-iso\n```\nCreates a bootable ISO with:\n- Minimal DWM desktop environment\n- Auto-login as 'nixos' user\n- Floorp browser auto-launching to Gradio UI\n- SLM-Assist with 45-second delayed start\n- Pre-baked corpus at /var/lib/slm-assist\n\n## Graphical ISO with Voice Pipeline\n```bash\nnix build .#packages.x86_64-linux.graphical-voice-iso\n```\nAdds voice interaction capabilities to the graphical ISO.\n\n## Headless Kexec Bundle\n```bash\nnix build .#packages.x86_64-linux.headless-kexec\n```\nFor booting into NixOS from an existing Linux system without rebooting.\n\n## Headless VM Image (qcow2)\n```bash\nnix build .#packages.x86_64-linux.headless-vm\n```\nQEMU/KVM-compatible disk image for virtual machine deployment.\n\n## Headless Raw Disk Image\n```bash\nnix build .#packages.x86_64-linux.headless-raw\n```\nRaw disk image for writing directly to storage devices.", "source": "build_targets", "chunk_index": 0, "total_chunks": 1}
{"text": "# SLM-Assist Configuration\n\nSLM-Assist is the core RAG (Retrieval-Augmented Generation) system in SLM-Boot.\n\n## Configuration Options\n```nix\nservices.slm-assist = {\n  enable = true;                              # Enable the service\n  ollamaModel = \"qwen3:0.6b-instruct-q5_K_M\"; # Model to use\n  gradioPort = 7861;                          # Web UI port\n  dataDir = \"/var/lib/slm-assist\";            # Data directory\n  exposeExternally = false;                   # Firewall setting\n  delayStartSec = 45;                         # Startup delay in seconds\n  autoOpenBrowser = true;                     # Auto-launch Floorp (graphical only)\n};\n```\n\n## How It Works\n1. **Ollama Service**: Native NixOS service runs Ollama LLM server on port 11434\n2. **Model Pre-pull**: Systemd service pre-downloads the specified model during boot\n3. **Delayed Start**: Timer waits 45 seconds for Ollama to initialize\n4. **Gradio Launch**: Python application starts serving the RAG interface\n5. **Browser Launch**: Floorp opens to http://127.0.0.1:7861 (graphical profiles only)\n\n## Architecture\n- **Backend**: Ollama provides LLM inference\n- **Framework**: DSPy for RAG pipeline orchestration\n- **Embeddings**: sentence-transformers for document encoding\n- **Vector Store**: FAISS for similarity search\n- **Frontend**: Gradio web interface\n- **Corpus**: Pre-baked JSONL file with domain knowledge", "source": "slm_assist_config", "chunk_index": 0, "total_chunks": 1}
{"text": "# SLM-Boot Module System\n\nThe system uses composable NixOS modules for flexibility.\n\n## Graphical Profile Modules\n```nix\ngraphicalModules = [\n  installation-cd-minimal.nix     # Base ISO configuration\n  graphical-minimal               # DWM desktop\n  preload                         # Preload common libraries\n  containers-base                 # Podman setup\n  open-webui-service              # Model interaction UI\n  hardening                       # Security hardening\n  production-extras               # Performance tuning\n  rag-dataset-tool                # Corpus preparation tool\n  kernel-cachyos-bore             # BORE kernel\n  slm-assist                      # Main RAG system\n];\n```\n\n## Headless Profile Modules\n```nix\nheadlessModules = [\n  headless-minimal                # Server base config\n  preload                         # Preload common libraries\n  containers-base                 # Podman setup\n  open-webui-service              # Model interaction UI\n  headless-access                 # SSH and remote access\n  hardening                       # Security hardening\n  production-extras               # Performance tuning\n  rag-dataset-tool                # Corpus preparation tool\n  kernel-cachyos-bore             # BORE kernel\n  slm-assist                      # Main RAG system (no browser)\n];\n```\n\n## Key Differences\n- **Graphical**: Includes desktop environment, auto-login, browser launch\n- **Headless**: SSH access, no GUI, service-only deployment\n- **Both**: Share core AI infrastructure (Ollama, SLM-Assist, corpus)", "source": "module_system", "chunk_index": 0, "total_chunks": 1}
{"text": "# Corpus Management in SLM-Boot\n\nThe corpus is the knowledge base used for RAG queries.\n\n## Corpus Location\n- **Source**: `modules/slm-assist/corpus/ragqa_arena_tech_corpus.jsonl`\n- **Runtime**: `/var/lib/slm-assist/ragqa_arena_tech_corpus.jsonl`\n\n## Corpus Format (JSONL)\nEach line is a JSON object with document chunks:\n```json\n{\"text\": \"Document content here...\", \"source\": \"filename.md\", \"chunk_index\": 0, \"total_chunks\": 3}\n```\n\n## How It's Integrated\nThe corpus is baked into the image using systemd tmpfiles:\n```nix\nsystemd.tmpfiles.rules = [\n  \"d /var/lib/slm-assist 0755 slm-assist slm-assist - -\"\n  \"C /var/lib/slm-assist/ragqa_arena_tech_corpus.jsonl - - - - ${./corpus/ragqa_arena_tech_corpus.jsonl}\"\n  \"Z /var/lib/slm-assist 0755 slm-assist slm-assist - -\"\n];\n```\n\n## Creating Your Own Corpus\nUse the included `rag_dataset_prep.py` script:\n```bash\npython3 scripts/rag_dataset_prep.py   your_docs/   /tmp/output   --corpus-output modules/slm-assist/corpus/ragqa_arena_tech_corpus.jsonl\n\ngit add modules/slm-assist/corpus/ragqa_arena_tech_corpus.jsonl\nnix build .#packages.x86_64-linux.graphical-iso\n```\n\n## Supported Source Formats\n- HTML/HTM files\n- PDF documents\n- Markdown files\n- Plain text files\n- JSONL files (for corpus-to-corpus conversion)\n- Web URLs (requires internet during build)", "source": "corpus_management", "chunk_index": 0, "total_chunks": 1}
{"text": "# SLM-Boot Development Workflow\n\n## Quick Start\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd SLM-Boot\n\n# Prepare your corpus\nmkdir -p modules/slm-assist/corpus\npython3 scripts/rag_dataset_prep.py docs/ /tmp/output   --corpus-output modules/slm-assist/corpus/ragqa_arena_tech_corpus.jsonl\n\n# Stage the corpus (required for Nix flakes)\ngit add modules/slm-assist/corpus/\n\n# Build the graphical ISO\nnix build .#packages.x86_64-linux.graphical-iso\n\n# Result is in ./result/iso/\nls -lh result/iso/*.iso\n```\n\n## Testing in a VM\n```bash\n# Build VM image\nnix build .#packages.x86_64-linux.headless-vm\n\n# Run with QEMU\nqemu-system-x86_64 -m 4G -smp 2 -enable-kvm   -drive file=result/nixos.qcow2,format=qcow2\n```\n\n## Writing to USB\n```bash\n# After building ISO\nsudo dd if=result/iso/nixos-*.iso of=/dev/sdX bs=4M status=progress\nsync\n```\n\n## Common Issues\n1. **Hash mismatch errors**: Run `nix hash to-sri --type sha256 <old-hash>`\n2. **Git tree warnings**: Stage files with `git add` (don't need to commit)\n3. **Missing corpus**: Ensure corpus file exists and is staged in Git\n4. **Service conflicts**: Check for duplicate service definitions across modules", "source": "development_workflow", "chunk_index": 0, "total_chunks": 1}
{"text": "# Ollama Integration in SLM-Boot\n\nOllama provides local LLM inference for the RAG system.\n\n## Configuration\nThe native NixOS Ollama service is used:\n```nix\nservices.ollama = {\n  enable = true;\n  # Optional GPU acceleration:\n  # package = pkgs.ollama-cuda;   # NVIDIA\n  # package = pkgs.ollama-rocm;   # AMD\n};\n```\n\n## Model Pre-pulling\nA systemd service automatically downloads the model:\n```nix\nsystemd.services.\"ollama-prepull-${cfg.ollamaModel}\" = {\n  description = \"Pre-pull Ollama model for SLM Assist\";\n  wantedBy = [ \"multi-user.target\" ];\n  after = [ \"ollama.service\" ];\n  serviceConfig = {\n    Type = \"oneshot\";\n    ExecStart = \"${pkgs.ollama}/bin/ollama pull qwen3:0.6b-instruct-q5_K_M\";\n    RemainAfterExit = true;\n    User = \"ollama\";\n    Group = \"ollama\";\n  };\n};\n```\n\n## Supported Models\n- **qwen3:0.6b-instruct-q5_K_M**: Default, very fast on CPU\n- **qwen3:4b-instruct-q5_K_M**: Better quality, needs more RAM\n- **llama3.1:8b**: High quality, requires 8GB+ RAM\n- **phi4:mini**: Microsoft's efficient model\n- Custom models from Ollama registry\n\n## API Access\n- **URL**: http://127.0.0.1:11434\n- **Protocol**: OpenAI-compatible REST API\n- **Environment**: Set via OLLAMA_HOST in service configs", "source": "ollama_integration", "chunk_index": 0, "total_chunks": 1}
{"text": "# DSPy RAG Pipeline in SLM-Assist\n\nDSPy orchestrates the Retrieval-Augmented Generation pipeline.\n\n## Pipeline Architecture\n1. **Query Processing**: User question received via Gradio\n2. **Embedding Generation**: sentence-transformers encodes the query\n3. **Vector Search**: FAISS finds most relevant corpus chunks\n4. **Context Assembly**: Top-k documents assembled into context\n5. **LLM Generation**: Ollama generates response using context\n6. **Response Rendering**: Answer displayed in Gradio UI\n\n## Key Components\n```python\n# Simplified DSPy pipeline structure\nclass RAGPipeline(dspy.Module):\n    def __init__(self):\n        self.retrieve = dspy.Retrieve(k=5)\n        self.generate = dspy.ChainOfThought(\"context, question -> answer\")\n    \n    def forward(self, question):\n        context = self.retrieve(question).passages\n        return self.generate(context=context, question=question)\n```\n\n## Python Dependencies\n- **dspy-ai**: Pipeline orchestration framework\n- **faiss-cpu**: Fast similarity search (CPU-only version)\n- **sentence-transformers**: Text embedding models\n- **gradio**: Web interface framework\n- **ujson**: Fast JSON parsing\n- **numpy**: Numerical operations\n\n## Index Management\nFAISS index is built at runtime:\n1. Corpus loaded from JSONL file\n2. Documents embedded using sentence-transformers\n3. FAISS index constructed in memory\n4. Queries search index for relevant chunks", "source": "dspy_pipeline", "chunk_index": 0, "total_chunks": 1}
{"text": "# CachyOS BORE Kernel in SLM-Boot\n\nThe BORE (Burst-Oriented Response Enhancer) kernel improves interactivity.\n\n## Why BORE?\nAI workloads create bursty CPU usage patterns. The BORE scheduler:\n- Prioritizes interactive tasks during burst periods\n- Maintains responsiveness under AI inference loads\n- Reduces latency for user input during model generation\n- Optimizes for desktop/workstation use cases\n\n## Configuration\n```nix\nmodules/kernel-cachyos-bore.nix:\n  boot.kernel.cachyos-bore.enable = true;\n\nflake.nix:\n  self.nixosModules.kernel-cachyos-bore\n  { boot.kernel.cachyos-bore.enable = true; }\n```\n\n## Benefits for AI Workloads\n1. **Responsive UI**: Gradio interface stays responsive during inference\n2. **Better Multitasking**: Run multiple models or applications smoothly\n3. **Reduced Jitter**: More consistent frame times in graphical environments\n4. **Optimized Scheduling**: AI tasks get CPU time without starving the UI\n\n## Comparison to Stock Kernel\n- **Stock**: CFS (Completely Fair Scheduler) treats all tasks equally\n- **BORE**: Detects burst patterns and prioritizes interactive tasks\n- **Result**: Better perceived performance for desktop AI usage", "source": "bore_kernel", "chunk_index": 0, "total_chunks": 1}
{"text": "# RAM (Random Access Memory) Architecture - Deep Dive\n\nRAM is the primary volatile memory system in computers, providing fast data access for running programs.\n\n## Physical Architecture\n\n### Memory Hierarchy\n1. **CPU Registers**: 1-2 cycles, <1KB total\n2. **L1 Cache**: 3-4 cycles, 32-64KB per core\n3. **L2 Cache**: 10-20 cycles, 256KB-1MB per core\n4. **L3 Cache**: 40-75 cycles, 8-64MB shared\n5. **Main Memory (RAM)**: 100-300 cycles, GB-TB scale\n6. **Storage (SSD/HDD)**: Millions of cycles, persistent\n\n### DRAM Technology\nModern RAM uses Dynamic RAM (DRAM):\n- **Storage Cell**: One transistor + one capacitor per bit\n- **Dynamic**: Capacitors leak charge, requiring periodic refresh (every 64ms)\n- **Density**: High bit density due to simple cell structure\n- **Cost**: Lower cost per bit than SRAM (used in caches)\n\n### DDR (Double Data Rate) Evolution\n- **DDR3**: 800-2133 MT/s, 1.5V, legacy systems\n- **DDR4**: 1600-3200 MT/s, 1.2V, current mainstream\n- **DDR5**: 4800-6400+ MT/s, 1.1V, modern high-end\n- **LPDDR**: Low-power variants for mobile devices\n\n## Memory Organization\n\n### Physical Layout\n```\nMemory Module (DIMM)\n├── Rank 0 (set of chips accessed together)\n│   ├── Chip 0 (8 bits)\n│   ├── Chip 1 (8 bits)\n│   └── ... (total 64 bits data + 8 bits ECC)\n└── Rank 1 (optional second set)\n```\n\n### Channels and Banks\n- **Channels**: Independent memory pathways (dual/quad/octa-channel)\n- **Banks**: Subdivisions within each memory chip\n- **Rows/Columns**: Address structure within banks\n- **Pages**: Row buffer holds recently accessed row (2-4KB)\n\n### Addressing Structure\nPhysical address broken into:\n1. **Channel bits**: Which memory channel\n2. **DIMM bits**: Which physical module\n3. **Rank bits**: Which rank on the module\n4. **Bank bits**: Which bank group and bank\n5. **Row bits**: Which row to activate\n6. **Column bits**: Which columns within the row\n\n## Memory Controller Operations\n\n### Access Patterns\n1. **Row Activation (RAS)**: Open a row into row buffer (~15-20ns)\n2. **Column Access (CAS)**: Read/write specific columns (~15ns)\n3. **Precharge**: Close row, prepare for next access (~15ns)\n4. **Refresh**: Periodically recharge all capacitors (64ms cycle)\n\n### Latency Components\nTotal latency = CAS Latency + RAS to CAS + Row Precharge + Command Rate\nExample DDR4-3200 CL16: 16 cycles CL + other timing = ~45ns actual latency\n\n### Bandwidth vs Latency\n- **Bandwidth**: How much data per second (GB/s) - helps sequential access\n- **Latency**: Time to first byte (ns) - critical for random access\n- **Parallelism**: Multiple outstanding requests improve throughput\n\n## Virtual Memory System\n\n### Memory Management Unit (MMU)\nThe MMU translates virtual addresses to physical addresses:\n- **Page Tables**: Multi-level tree structure (4-level on x86-64)\n- **Page Size**: Typically 4KB, with 2MB/1GB huge pages available\n- **TLB (Translation Lookaside Buffer)**: Caches recent translations\n\n### Virtual Address Space\n64-bit systems provide huge virtual address space:\n- User space: 0x0000000000000000 - 0x00007FFFFFFFFFFF (128TB)\n- Kernel space: 0xFFFF800000000000 - 0xFFFFFFFFFFFFFFFF (128TB)\n- Actual usable: Limited by physical RAM + swap\n\n### Page Faults\nWhen accessing non-resident pages:\n1. **Minor Fault**: Page not in RAM but allocated (fetch from disk/zero-fill)\n2. **Major Fault**: Page must be read from storage (very slow, ~1-10ms)\n3. **Segmentation Fault**: Invalid address access (program error)", "source": "ram_architecture_part1", "chunk_index": 0, "total_chunks": 2}
{"text": "# RAM Management in Linux and NixOS\n\nLinux provides sophisticated memory management optimized for various workloads.\n\n## Page Cache and Buffer Cache\n\n### Page Cache\nThe kernel caches file contents in RAM:\n- **Purpose**: Avoid slow disk I/O by keeping frequently accessed files in memory\n- **Automatic**: Kernel uses free RAM for caching, releases under pressure\n- **Benefits**: Dramatically speeds up repeated file access\n- **Cost**: \"Used\" memory shown in `free` includes cache (actually available)\n\n### Buffer Cache\nCaches block device metadata:\n- **Inodes**: File metadata (permissions, timestamps, location)\n- **Directory entries**: Cached directory listings\n- **Superblocks**: Filesystem metadata\n- **Importance**: Critical for filesystem performance\n\n### Memory Pressure\nWhen applications need RAM:\n1. Kernel identifies least-recently-used (LRU) pages\n2. Clean pages dropped immediately (can reload from disk)\n3. Dirty pages written to disk first, then dropped\n4. Cache shrinks, application gets memory\n\n## Memory Allocators\n\n### Kernel Allocator (SLUB)\nManages kernel memory:\n- **SLUB allocator**: Modern default (replaced SLAB and SLOB)\n- **Object caching**: Frequently allocated structures cached\n- **Per-CPU caches**: Reduces lock contention\n- **Slab pages**: Groups objects by size class\n\n### User Space Allocators\nApplications use allocators like glibc malloc:\n- **malloc/free**: Standard C allocation functions\n- **Memory arenas**: Per-thread allocation regions\n- **mmap**: Large allocations mapped directly\n- **jemalloc/tcmalloc**: Alternative allocators for performance\n\n## Memory Overcommit\n\n### Overcommit Modes\n```bash\n/proc/sys/vm/overcommit_memory:\n0 = Heuristic (default): Allow reasonable overcommit\n1 = Always: Allow all allocations (can lead to OOM)\n2 = Never: Strict accounting, no overcommit\n```\n\n### Why Overcommit?\n- Applications often allocate more than they use\n- fork() creates copy-on-write duplicates\n- Allows more processes to run with limited RAM\n- Risk: Out-of-Memory (OOM) killer may terminate processes\n\n## OOM (Out of Memory) Killer\n\n### When OOM Triggers\n1. System exhausts RAM and swap\n2. Kernel cannot satisfy allocation request\n3. OOM killer selects victim process\n4. Process terminated to free memory\n\n### OOM Score\nEach process has OOM score (0-1000):\n- Based on memory usage, runtime, and importance\n- Lower score = less likely to be killed\n- Adjustable via `/proc/<pid>/oom_score_adj`\n- System processes protected by default\n\n## Memory Limits and Control Groups\n\n### cgroups v2 Memory Controller\n```bash\n# Set hard limit\necho \"4G\" > /sys/fs/cgroup/myapp/memory.max\n\n# Set soft limit (pressure point)\necho \"3G\" > /sys/fs/cgroup/myapp/memory.high\n\n# Monitor usage\ncat /sys/fs/cgroup/myapp/memory.current\n```\n\n### NixOS Service Limits\n```nix\nsystemd.services.myservice = {\n  serviceConfig = {\n    MemoryMax = \"4G\";        # Hard limit\n    MemoryHigh = \"3.5G\";     # Soft limit (throttling)\n    MemorySwapMax = \"2G\";    # Swap limit\n  };\n};\n```\n\n## Swap and Swappiness\n\n### Swap Space\nExtends RAM using disk storage:\n- **Swap Partition**: Dedicated disk partition\n- **Swap File**: Regular file used as swap\n- **zram**: Compressed RAM used as swap (fast but uses RAM)\n\n### Swappiness Parameter\n```bash\n/proc/sys/vm/swappiness (0-200, default 60):\n0 = Avoid swap unless necessary\n60 = Balanced (default)\n100 = Aggressive swapping\n200 = Prefer swapping over page cache\n```\n\n### SLM-Boot Considerations\nFor AI workloads:\n- **Lower swappiness (10-20)**: Keep model weights in RAM\n- **Large swap on SSD**: Graceful degradation if OOM\n- **zram**: Fast compressed swap for temporary pressure\n- **Monitor pressure**: Watch for thrashing (excessive swapping)\n\n## NUMA (Non-Uniform Memory Access)\n\n### Architecture\nMulti-socket systems have local RAM per CPU:\n- **Local**: Fast access to CPU's own RAM\n- **Remote**: Slower access to other CPU's RAM\n- **Latency**: Remote access 1.5-3x slower\n\n### NUMA Policy\n```bash\n# Bind process to node 0\nnumactl --cpunodebind=0 --membind=0 ./myapp\n\n# Interleave across all nodes\nnumactl --interleave=all ./myapp\n```\n\n### NixOS NUMA Configuration\nKernel automatically handles NUMA:\n- **AutoNUMA**: Migrates pages to local memory\n- **Transparent**: Usually no tuning needed\n- **Monitoring**: `numastat` shows NUMA statistics", "source": "ram_management_linux", "chunk_index": 1, "total_chunks": 2}
{"text": "# RAM Optimization for AI/LLM Workloads\n\nAI inference and training have unique memory requirements and optimization strategies.\n\n## LLM Memory Requirements\n\n### Model Size Calculations\n```\nFormula: Parameters × Bytes_per_Parameter × Overhead\n\nExamples (FP16/Q8 quantization):\n- 0.6B parameters × 2 bytes = 1.2GB (+ ~20% overhead = 1.5GB)\n- 4B parameters × 2 bytes = 8GB (+ ~20% overhead = 10GB)\n- 8B parameters × 2 bytes = 16GB (+ ~20% overhead = 20GB)\n- 70B parameters × 2 bytes = 140GB (+ ~20% overhead = 170GB)\n\nQuantization reduces size:\n- FP32: 4 bytes per parameter (full precision)\n- FP16: 2 bytes per parameter (half precision)\n- Q8: 1 byte per parameter (8-bit quantization)\n- Q4: 0.5 bytes per parameter (4-bit quantization)\n```\n\n### Memory Components\n1. **Model Weights**: Largest component, size depends on quantization\n2. **KV Cache**: Stores attention keys/values for context (grows with context length)\n3. **Activations**: Intermediate computations during inference\n4. **Gradients**: Only during training (not needed for inference)\n\n### Context Window Impact\nKV cache grows with context length:\n```\nKV Cache Size = Layers × Heads × Head_Dim × Context_Length × 2 × Bytes_per_Element\n\nExample (Llama-style 8B model, 8K context, FP16):\n32 layers × 32 heads × 128 head_dim × 8192 ctx × 2 (K+V) × 2 bytes\n= ~2GB KV cache for 8K context\n= ~8GB KV cache for 32K context\n```\n\n## Memory Optimization Techniques\n\n### Quantization\nReduces precision to save memory:\n- **Q8_0**: 8-bit integer, good quality, 75% reduction vs FP32\n- **Q5_K_M**: 5-bit mixed precision, balanced quality/size\n- **Q4_K_M**: 4-bit mixed precision, 87.5% reduction vs FP32\n- **Q3**: Aggressive compression, noticeable quality loss\n- **GGUF format**: Optimized quantized format (used by Ollama)\n\n### Model Offloading\nSplit model across devices:\n- **GPU Layers**: Most layers on GPU (fast)\n- **CPU Layers**: Remaining layers on CPU (slower)\n- **Disk Offloading**: Rarely used layers paged from disk (very slow)\n- **Ollama**: Automatically manages GPU memory, spills to CPU/disk\n\n### Batching Strategies\n- **Static Batching**: Process fixed batch size\n- **Dynamic Batching**: Accumulate requests until batch size or timeout\n- **Continuous Batching**: Stream tokens while processing next requests\n- **Memory Trade-off**: Larger batches = better throughput but more RAM\n\n### KV Cache Management\n- **Sliding Window**: Keep only recent tokens (limited context)\n- **Token Eviction**: Remove least important tokens (attention-based)\n- **Chunked Processing**: Process long documents in segments\n- **Recomputation**: Recompute instead of cache (time vs memory trade-off)\n\n## SLM-Boot Memory Configuration\n\n### Systemd Service Limits\n```nix\nsystemd.services.ollama = {\n  serviceConfig = {\n    MemoryMax = \"4G\";         # Prevent runaway memory usage\n    MemoryHigh = \"3.5G\";      # Start throttling here\n  };\n};\n```\n\n### Ollama Environment Variables\n```bash\nOLLAMA_MAX_LOADED_MODELS=1    # Only keep 1 model in RAM\nOLLAMA_NUM_PARALLEL=1         # Process 1 request at a time\nOLLAMA_MAX_QUEUE=10           # Queue up to 10 requests\n```\n\n### Model Selection by RAM\n```\nAvailable RAM → Recommended Models:\n4GB: qwen3:0.6b (Q5_K_M), phi4:mini\n8GB: qwen3:4b (Q5_K_M), llama3.1:3b, mistral:7b (Q4)\n16GB: llama3.1:8b (Q5_K_M), mistral:7b (Q8), command-r:7b\n32GB+: llama3.1:70b (Q4), mixtral:8x7b, larger models\n```\n\n## Monitoring and Troubleshooting\n\n### Memory Pressure Indicators\n```bash\n# Check overall memory usage\nfree -h\n\n# Watch for swapping\nvmstat 1\n\n# Memory pressure stalls (cgroups v2)\ncat /proc/pressure/memory\n\n# Per-process memory\nps aux --sort=-%mem | head\n\n# Ollama-specific\njournalctl -u ollama -f\n```\n\n### Warning Signs\n1. **High Swap Usage**: Model weights swapping out (very slow)\n2. **OOM Kills**: Processes terminated unexpectedly\n3. **Slow Inference**: Excessive paging, thrashing\n4. **Failed Allocations**: Model won't load at all\n\n### Solutions\n1. **Use Smaller Model**: Reduce parameter count or quantization\n2. **Reduce Context**: Shorter prompts = less KV cache\n3. **Add More RAM**: Hardware upgrade if needed\n4. **Use zram**: Fast compressed swap as safety net\n5. **Enable Offloading**: Let Ollama spill to disk gracefully\n\n## Best Practices for SLM-Boot\n\n### Graphical ISO (Live USB)\n- 8GB RAM minimum: Supports qwen3:0.6b + desktop\n- 16GB RAM recommended: Better experience, larger models possible\n- zram swap: Compressed RAM swap for safety\n\n### Headless Deployment\n- 4GB RAM minimum: Headless + qwen3:0.6b\n- No desktop overhead: More RAM available for models\n- Swap on SSD: Graceful degradation\n\n### Production Considerations\n1. Monitor memory pressure continuously\n2. Set conservative MemoryMax limits\n3. Use appropriate model for available RAM\n4. Test under realistic concurrent load\n5. Plan for peak usage, not average", "source": "ram_optimization_ai", "chunk_index": 2, "total_chunks": 2}
